{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|             article|          highlights|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|0001d1afc246a7964...|By . Associated P...|Bishop John Folda...|\n",
      "|He contracted the...|                null|                null|\n",
      "|Church members in...| Grand Forks and ...|                null|\n",
      "|0002095e55fcbd3a2...|\"(CNN) -- Ralph M...|\"\" of using his r...|\n",
      "|          Ralph Mata| an internal affa...| allegedly helped...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|             article|          highlights|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|92c514c913c0bdfe2...|Ever noticed how ...|Experts question ...|\n",
      "|U.S consumer advi...|                null|                null|\n",
      "|Safety tests cond...|                null|                null|\n",
      "|2003841c7dc0e7c5b...|A drunk teenage b...|Drunk teenage boy...|\n",
      "|         Rahul Kumar|                  17| ran towards anim...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                  id|             article|          highlights|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|61df4979ac5fcc2b7...|Sally Forrest, an...|Sally Forrest, an...|\n",
      "|             Forrest| whose birth name...| had long battled...|\n",
      "|  A San Diego native| Forrest became a...| who cast her in ...|\n",
      "|21c0bd69b7e7df285...|A middle-school t...|Works include pic...|\n",
      "|         Has inked 1|000 pieces of art...|                null|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SummarizationApp\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define the schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"article\", StringType(), True),\n",
    "    StructField(\"highlights\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV files in HDFS using the defined schema\n",
    "train_data = spark.read.schema(schema).option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/train.csv\")\n",
    "test_data = spark.read.schema(schema).option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/test.csv\")\n",
    "validation_data = spark.read.schema(schema).option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/validation.csv\")\n",
    "\n",
    "# Show the first 5 rows of each dataset (optional)\n",
    "train_data.show(5)\n",
    "test_data.show(5)\n",
    "validation_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the \"id\" column from the dataframes\n",
    "train_data = train_data.drop(\"id\")\n",
    "test_data = test_data.drop(\"id\")\n",
    "validation_data = validation_data.drop(\"id\")\n",
    "\n",
    "# Drop rows with missing values in the \"article\" or \"highlights\" columns\n",
    "train_data = train_data.dropna(subset=[\"article\", \"highlights\"])\n",
    "test_data = test_data.dropna(subset=[\"article\", \"highlights\"])\n",
    "validation_data = validation_data.dropna(subset=[\"article\", \"highlights\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             article|          highlights|\n",
      "+--------------------+--------------------+\n",
      "|by   associated p...|bishop john folda...|\n",
      "|cnn     ralph mat...|of using his role...|\n",
      "|an internal affai...|allegedly helped ...|\n",
      "|a drunk driver wh...|craig eccleston t...|\n",
      "|cnn     with a br...|nina dos santos s...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|             article|          highlights|\n",
      "+--------------------+--------------------+\n",
      "|ever noticed how ...|experts question ...|\n",
      "|a drunk teenage b...|drunk teenage boy...|\n",
      "|                  17|ran towards anima...|\n",
      "|dougie freedman i...|nottingham forest...|\n",
      "|liverpool target ...|fiorentina goalke...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|             article|          highlights|\n",
      "+--------------------+--------------------+\n",
      "|sally forrest  an...|sally forrest  an...|\n",
      "|whose birth name ...|had long battled ...|\n",
      "|forrest became a ...|who cast her in s...|\n",
      "|a middle school t...|works include pic...|\n",
      "|a man convicted o...|iftekhar murtaza ...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "\n",
    "# Define a function to clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "# UDF for PySpark DataFrame\n",
    "clean_udf = udf(clean_text, StringType())\n",
    "\n",
    "# Apply cleaning to the datasets\n",
    "train_data = train_data.withColumn(\"article\", clean_udf(train_data[\"article\"])).withColumn(\"highlights\", clean_udf(train_data[\"highlights\"]))\n",
    "test_data = test_data.withColumn(\"article\", clean_udf(test_data[\"article\"])).withColumn(\"highlights\", clean_udf(test_data[\"highlights\"]))\n",
    "validation_data = validation_data.withColumn(\"article\", clean_udf(validation_data[\"article\"])).withColumn(\"highlights\", clean_udf(validation_data[\"highlights\"]))\n",
    "# Show the first 5 rows of dataframes\n",
    "train_data.show(5)\n",
    "test_data.show(5)\n",
    "validation_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Row Count: 351784\n",
      "Test Data Row Count: 14149\n",
      "Validation Data Row Count: 16586\n"
     ]
    }
   ],
   "source": [
    "# Print the row count of each dataframe\n",
    "print(\"Train Data Row Count:\", train_data.count())\n",
    "print(\"Test Data Row Count:\", test_data.count())\n",
    "print(\"Validation Data Row Count:\", validation_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 15:=============================================>           (8 + 2) / 10]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Row Count: 1000\n",
      "Test Data Row Count: 250\n",
      "Validation Data Row Count: 250\n"
     ]
    }
   ],
   "source": [
    "# Define the desired row limits for each dataset\n",
    "row_limits = {\n",
    "    \"train_data\": 1000,\n",
    "    \"test_data\": 250,\n",
    "    \"validation_data\": 250\n",
    "}\n",
    "\n",
    "# Limit each dataset to the specified number of rows\n",
    "for dataset_name, row_limit in row_limits.items():\n",
    "    globals()[dataset_name] = globals()[dataset_name].limit(row_limit)\n",
    "\n",
    "print(\"Train Data Row Count:\", train_data.count())\n",
    "print(\"Test Data Row Count:\", test_data.count())\n",
    "print(\"Validation Data Row Count:\", validation_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Transformation and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-10 20:43:07.754844: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-10 20:43:08.257385: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-10 20:43:08.257411: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-10 20:43:08.320055: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-10 20:43:09.625053: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-10 20:43:09.625344: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-10 20:43:09.625354: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Traceback (most recent call last):===============>                 (7 + 2) / 10]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "\n",
    "# Fit the tokenizer on the training data (both articles and highlights)\n",
    "all_train_texts = train_data.select(\"article\").rdd.flatMap(lambda x: x).collect() + train_data.select(\"highlights\").rdd.flatMap(lambda x: x).collect()\n",
    "tokenizer.fit_on_texts(all_train_texts)\n",
    "\n",
    "# Convert text data into sequences of integers\n",
    "train_articles_seq = tokenizer.texts_to_sequences(train_data.select(\"article\").rdd.flatMap(lambda x: x).collect())\n",
    "train_highlights_seq = tokenizer.texts_to_sequences(train_data.select(\"highlights\").rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "test_articles_seq = tokenizer.texts_to_sequences(test_data.select(\"article\").rdd.flatMap(lambda x: x).collect())\n",
    "test_highlights_seq = tokenizer.texts_to_sequences(test_data.select(\"highlights\").rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "val_articles_seq = tokenizer.texts_to_sequences(validation_data.select(\"article\").rdd.flatMap(lambda x: x).collect())\n",
    "val_highlights_seq = tokenizer.texts_to_sequences(validation_data.select(\"highlights\").rdd.flatMap(lambda x: x).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the maximum lengths for sequences\n",
    "MAX_LEN_ARTICLE = 1768\n",
    "MAX_LEN_HIGHLIGHT = 1769\n",
    "\n",
    "# Perform padding\n",
    "train_articles_seq = pad_sequences(train_articles_seq, maxlen=MAX_LEN_ARTICLE, padding=\"post\", truncating=\"post\")\n",
    "train_highlights_seq = pad_sequences(train_highlights_seq, maxlen=MAX_LEN_HIGHLIGHT, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "test_articles_seq = pad_sequences(test_articles_seq, maxlen=MAX_LEN_ARTICLE, padding=\"post\", truncating=\"post\")\n",
    "test_highlights_seq = pad_sequences(test_highlights_seq, maxlen=MAX_LEN_HIGHLIGHT, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "val_articles_seq = pad_sequences(val_articles_seq, maxlen=MAX_LEN_ARTICLE, padding=\"post\", truncating=\"post\")\n",
    "val_highlights_seq = pad_sequences(val_highlights_seq, maxlen=MAX_LEN_HIGHLIGHT, padding=\"post\", truncating=\"post\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: fracking firms will be allowed to access vast reserves of underground gas without the permission of landowners under controversial laws being drawn up by the government  ministers are preparing an overhaul of trespass legislation to make it easier for firms to ignore objections  one source said the reform  which will infuriate anti fracking campaigners  was likely to be included in the queen s speech setting out the government s plans for its final year  anger  police tackle protesters at balcombe  west sussex  who were objecting to a test drilling site there   chancellor george osborne is offering generous tax breaks to kickstart the technology  he believes fracking for shale gas could herald an energy revolution that will boost the economy  make britain more self sufficient and put an end to sky high bills from greedy energy firms  scientists say the uk is sitting on deposits of enough shale gas to supply the whole country for at least 40 years  mirroring the north sea oil boom of the seventies  shale gas development has taken off in the us  using the controversial process of fracking   or hydraulic fracturing  underground gas deposits are extracted by fracturing shale rock with high pressure blasts of water  sand and chemicals  opponents warn that the process risks causing earthquakes  polluting water  blighting the countryside and affecting house prices  shale gas exploration involves sinking   a vertical well and exploring out from it horizontally  often for more   than a mile  environmental campaigners and residents have demonstrated   against fracking  clashing with police at well sites such as balcombe    west sussex  under current   law  firms need permission from owners of land over fracking tunnels  if   the owners object  a developer has to take them to court to overturn   their objections and agree compensation  resistance  police get to grips with another group of protesters at balcombe last summer  the reform  which will infuriate anti fracking campaigners  will make it easier to ignore local objections to drilling   landowners in the us own any shale gas beneath their land  but in britain it belongs to the crown   or  in practice  the government  ministers are issuing licences for developers applying to sink exploratory wells  typically  fracking involves horizontal exploration a mile or more from the central vertical well  meaning many different landowners could be involved  however  laws dating back to the mid sixties mean landowners must not withhold permission if drilling under their land is to go ahead  developers are able to try to overturn any such objection in court  but judges would consider why permission was withheld and whether reasonable efforts have been made to reach an accommodation  they could order compensation be paid even if they overturn an objection and cases can take years to resolve  creating an obstacle to development  greenpeace has started a campaign to get people in potential fracking sites to register objections to exploration under their land  landowners in the sussex downs national park  including city fund manager marcus adams  plan to use the existing law to block development  mr adams said   people across the country have legitimate concerns about the impact of fracking  from water contamination to air and noise pollution   but all this happening in a national park just doesn t bear thinking about   the reform of the law would extend the existing rights of water  gas and coal mining firms  set out in the coal act 1998  to go under people s land without permission  compensation of around  100 is likely to be offered to landowners  the revamp would also apply to the geothermal wells that harness heat from deep in the earth  developers need to drill on land in urban areas  with one proposed project in manchester likely to involve exploration under 5 000 homes  the department for energy and climate change said   operators prefer to agree through negotiation with the landowner  but there is an existing legal route by which they can apply for access where this can t be negotiated  we re considering whether this is fit for purpose\n",
      "Tokenized and Padded Article: [4741 2528   42 ...    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):===============>                 (7 + 2) / 10]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Highlight: scientists say uk is sitting on enough shale gas for 40 years  supply\n",
      "Tokenized and Padded Highlight: [1862  176  258 ...    0    0    0]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: by   kelly strange   published    10 55 est  18 october 2013       updated    10 57 est  18 october 2013   a woman has spoken of the heartbreaking moment she was told she was infertile because she had unprotected sex with one partner as a teenager  jodie watson  21  was undergoing exploratory tests in hospital to find out why she and her fianc  had been unable to conceive a baby when she was given the devastating news that she may never be a mother  she was stunned to be told her infertility had been caused by an std she had contracted and been treated for as a teenager  jodie watson  21  caught chlamydia when she was 16  when she started trying for a baby with her fianc  james jackson  30  she found the disease had left her infertile   now  as she struggles to raise money for ivf  ms watson has made the brave decision to talk about her heartbreak in the hope that it will stop other teenagers from repeating her mistake  she said   i was young and i made one foolish mistake that i will pay for forever   that single mistake will haunt me for the rest of my life and i want other young women to know that when they have unprotected sex they are dicing with their future and their fertility   my dreams have been destroyed because of something that happened five years ago but i want to talk about it to stop it happening to anyone else   ms watson thought she was being sensible when she protected herself against pregnancy before sleeping with a partner when she was 16  but after going for an sti test she was told she had contracted chlamydia  she was treated with antibiotics and put it behind her  she says   i d learned a horrible lesson at a young age  but what young woman doesn t make mistakes   i put it behind me and vowed it would never happen again   ms watson had treatment when she was 16 and thought she was cured  when she struggled to conceive tests revealed her fallopian tubes were irreversibly damaged   but ms watson was unaware that the damage had already been done  it wasn t until she met james jackson through her work in a care home  that she was to discover just how much her life had been affected by one mistake  the couple quickly became serious and mr jackson proposed  when they moved into their own home in hartlepool  county durham  the couple started trying for a baby  but after a year with no joy jodie saw her gp in april 2012  ms watson and her fianc  are now trying to raise the money needed to pay for ivf treatment   blood tests came back normal so she was referred to a gynaecologist in june 2012  she underwent a series of ultrasounds and x rays in october 2012  which revealed blockages in her tubes  ms watson said   i was stunned when they told us what they had found  but was not prepared at all when they said it was usually caused by chlamydia  i hadn t even told james about it because it had happened so long ago  i thought it was in the past   but now it was affecting her future dreams of motherhood and ms watson was told she was infertile   i broke down and cried  i just couldn t believe it  i was ashamed  embarrassed and shocked   i thought because i had been treated i would be fine but the consultant said the damage had likely already been done  i had no idea   their only hope of a family lies in ivf but as mr jackson had a daughter with an ex partner the couple do not qualify for treatment on the nhs  after being dealt the bombshell ms watson suffered with depression  she said   i was just blaming myself every day and wishing i could turn back the clock   after researching her condition she read about an operation  which could help to unblock her tubes and increase her chances of becoming pregnant  her gp explained there was just a 15 per cent chance it would work but gave the go ahead  ms watson underwent the operation in january this year but has still not become pregnant  ms watson wants to raise awareness about the dangers of unprotected sex as she says she regrets her decision every day and doesn t want other teenagers to make the same mistake    they managed to partially unblock the right tube but the left was too badly damaged   she explains   there is still a tiny chance i might fall pregnant so i cling to that but i know deep down the only real chance is likely ivf   ms watson applied to take part in an egg sharing scheme but was rejected because of the std  at between  4 000 and  6 000 per cycle the couple are now fundraising to pay for ivf  she says   after the bills there is hardly anything left to save so i m doing what i can to raise the money including car boot sales  ms watson has also started a facebook page to spread her warning to as many teens as possible  ms watson said   every day i regret the decision i made as a teenager but i know there are likely hundreds of other teenagers doing the same and i want to save them this heartbreak   i think it s unfair that i don t qualify for help on the nhs because my partner already has a child  that shouldn t stop me from being a mum  i would like to see that rule reviewed   every day i regret the decision i made as a teenager but i know there are likely hundreds of other teenagers doing the same and i want to save them this heartbreak   i know there are people who will judge me  but we all make mistakes and nobody regrets it more than me   i wish i had never had unprotected sex and i wish i had known the full extent of the damage that can be caused   i m praying for a miracle and hope that one day i will get the chance to be a mother   in the meantime i will do what i can to help other women by raising awareness   if it is not treated  chlamydia can cause long term problems  in women it can spread to the womb  ovaries and fallopian tubes   this can cause a condition called pelvic inflammatory disease  pid   pid can cause infertility and persistent pelvic pain  it also increases the risk of miscarriage and ectopic pregnancy  chlamydia can also spread to cause inflammation in the fallopian tubes   this can make it difficult for an egg to travel from the ovary to the womb making conception more difficult  in men  chlamydia can cause epididymitis   swelling of the part of the man s reproductive system that carries sperm from the testicles  if left untreated  it can lead to infertility\n",
      "Tokenized and Padded Article: [  21 1276 4510 ...    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):===============>                 (7 + 2) / 10]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Highlight: jodie watson caught chlamydia from a partner when she was 16\n",
      "Tokenized and Padded Highlight: [9001 2018  883 ...    0    0    0]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: robin emmons has grown more than 26\n",
      "Tokenized and Padded Article: [ 3077 14237    26 ...     0     0     0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):====>                            (5 + 3) / 10]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Highlight: 000 pounds of fruits and vegetables\n",
      "Tokenized and Padded Highlight: [  94 1098    6 ...    0    0    0]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: by   ted thornhill   last updated at 2 18 pm on 30th december 2011   bored of your surroundings and want to liven things up  or fancy destroying that broken bit of office machinery  now there s an app that ll add a bit of action movie magic into your life by super imposing dramatic special effects over mobile phone footage  action movie fx for iphone comes courtesy of j j  abrams  production company bad robot interactive  which is behind star trek  super 8 and mission impossible 3  scroll down for mailonline s blockbuster debut   kaboom  a mailonline reporter finds himself in the line of fire   dramatic  the app s effects are incredibly realistic   leave it to the professionals  the company behind the app  bad robot  has plenty of expertise with movie special effects   the free app comes with two effects   a missile strike and a car crash  and the results are incredible  simply hit record and film a scene  then use a slider to mark the point at which you want the  fx  to begin  the missile strike is gigantic and a burning pile of rubble is left behind after the initial explosion  the car crash is equally professional looking  swirls of dust precede an suv dropping out of the sky and flying into the camera  car nage  a terrifying car crash is one of the free effects that comes with the app   lights  iphone  action  for 99 cents you can add in a helicopter crash   gunning for filmmaking glory  the app also features fake armed police   simple  the app is straightforward to use and it only takes seconds before you ve made special effects laden footage   there are even action movie sound effects to accompany the carnage  more effects  such as a helicopter falling from the sky  can be added for a small fee  so far it s proving hugely popular  with one user  jimmo olson  commenting   i ordered a missile strike on my office calendar  looks like i m leaving early today\n",
      "Tokenized and Padded Article: [  21 2289 7330 ...    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):===============>                 (7 + 2) / 10]\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Highlight: the free iphone app comes with a  missile strike  and a  car crash\n",
      "Tokenized and Padded Highlight: [   2  514 1572 ...    0    0    0]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article: cnn     harvard is caught up in a student cheating scandal that its dean of undergraduate education calls   unprecedented in its scope and magnitude    as a harvard grad\n",
      "Tokenized and Padded Article: [ 128 5750   13 ...    0    0    0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 663, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Highlight: i am embarrassed\n",
      "Tokenized and Padded Highlight: [  22  436 4549 ...    0    0    0]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Select random indices\n",
    "random_idx = random.sample(range(len(train_articles_seq)), 5)\n",
    "# Display five randomly selected examples\n",
    "for idx in random_idx:\n",
    "    # Display the original article and its tokenized and padded version\n",
    "    print(\"Original Article:\", train_data.select(\"article\").rdd.flatMap(lambda x: x).collect()[idx])\n",
    "    print(\"Tokenized and Padded Article:\", train_articles_seq[idx])\n",
    "    # Display the original highlight and its tokenized and padded version\n",
    "    print(\"Original Highlight:\", train_data.select(\"highlights\").rdd.flatMap(lambda x: x).collect()[idx])\n",
    "    print(\"Tokenized and Padded Highlight:\", train_highlights_seq[idx])\n",
    "     # Draw a line between two items\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 24972\n"
     ]
    }
   ],
   "source": [
    "# Calculate the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Article Length: 1768.0\n",
      "Average Highlight Length: 1769.0\n",
      "Max Article Length: 1768\n",
      "Max Highlight Length: 1769\n"
     ]
    }
   ],
   "source": [
    "# Calculate the lengths of articles and highlights\n",
    "article_lengths = [len(seq) for seq in train_articles_seq]\n",
    "highlight_lengths = [len(seq) for seq in train_highlights_seq]\n",
    "\n",
    "# Calculate and print average article length\n",
    "print(f\"Average Article Length: {sum(article_lengths)/len(article_lengths)}\")\n",
    "\n",
    "# Calculate and print average highlight length\n",
    "print(f\"Average Highlight Length: {sum(highlight_lengths)/len(highlight_lengths)}\")\n",
    "\n",
    "# Calculate and print maximum article length\n",
    "print(f\"Max Article Length: {max(article_lengths)}\")\n",
    "\n",
    "# Calculate and print maximum highlight length\n",
    "print(f\"Max Highlight Length: {max(highlight_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty article sequences: 0\n",
      "Number of empty highlight sequences: 0\n"
     ]
    }
   ],
   "source": [
    "# Find empty article sequences (sequences with length 0)\n",
    "empty_articles = [seq for seq in train_articles_seq if len(seq) == 0]\n",
    "\n",
    "# Find empty highlight sequences (sequences with length 0)\n",
    "empty_highlights = [seq for seq in train_highlights_seq if len(seq) == 0]\n",
    "\n",
    "# Print the number of empty article sequences\n",
    "print(f\"Number of empty article sequences: {len(empty_articles)}\")\n",
    "\n",
    "# Print the number of empty highlight sequences\n",
    "print(f\"Number of empty highlight sequences: {len(empty_highlights)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles with OOV tokens: 0\n",
      "Number of highlights with OOV tokens: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the index of the OOV token in the word index\n",
    "oov_token_index = tokenizer.word_index[tokenizer.oov_token]\n",
    "\n",
    "# Count the number of articles containing OOV tokens\n",
    "oov_in_articles = sum([1 for seq in train_articles_seq if oov_token_index in seq])\n",
    "\n",
    "# Count the number of highlights containing OOV tokens\n",
    "oov_in_highlights = sum([1 for seq in train_highlights_seq if oov_token_index in seq])\n",
    "\n",
    "# Print the number of articles with OOV tokens\n",
    "print(f\"Number of articles with OOV tokens: {oov_in_articles}\")\n",
    "\n",
    "# Print the number of highlights with OOV tokens\n",
    "print(f\"Number of highlights with OOV tokens: {oov_in_highlights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 1768)]       0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 1768)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1768, 256)    6393088     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 1768, 256)    6393088     ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  [(None, 512),        1574912     ['embedding_3[0][0]']            \n",
      "                                 (None, 512),                                                     \n",
      "                                 (None, 512)]                                                     \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  [(None, 1768, 512),  1574912     ['embedding_4[0][0]',            \n",
      "                                 (None, 512),                     'lstm_4[0][1]',                 \n",
      "                                 (None, 512)]                     'lstm_4[0][2]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1768, 24973)  12811149    ['lstm_5[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 28,747,149\n",
      "Trainable params: 28,747,149\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 256  # Dimension of word embeddings\n",
    "HIDDEN_UNITS = 512   # Number of LSTM units\n",
    "\n",
    "# Vocabulary sizes - add 1 for padding token\n",
    "vocab_size_articles = 24972 + 1\n",
    "vocab_size_highlights = 24972 + 1\n",
    "\n",
    "# Maximum sequence lengths\n",
    "max_length_articles = 1768\n",
    "max_length_highlights = 1768\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_length_articles,))\n",
    "encoder_embedding = Embedding(vocab_size_articles, EMBEDDING_DIM)(encoder_inputs)\n",
    "encoder_lstm = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_length_highlights,))\n",
    "decoder_embedding_layer = Embedding(vocab_size_highlights, EMBEDDING_DIM)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(HIDDEN_UNITS, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size_highlights, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compilation\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# Dimension of word embeddings (a hyperparameter you can define)\n",
    "embedding_dim = 256\n",
    "\n",
    "# Create an embedding matrix and specify its dimensions\n",
    "embedding_layer = Embedding(vocab_size_articles, embedding_dim, input_length=max_length_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_length_articles,))\n",
    "encoder_embedding = embedding_layer(encoder_inputs)  # Transformation with the embedding matrix\n",
    "encoder_lstm = LSTM(HIDDEN_UNITS, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Create the encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define decoder inputs and initial_state\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_states_input = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Reuse the embedding layer and LSTM layer for the decoder\n",
    "decoder_embedding2 = embedding_layer(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(decoder_embedding2, initial_state=decoder_states_input)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# Reuse the output layer\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# Create the decoder model\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_input, [decoder_outputs2] + decoder_states2)\n",
    "\n",
    "# Now, combine the encoder and decoder models\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create and compile the overall model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Shape: (1000, 1768)\n",
      "Decoder Input Shape: (1000, 1768)\n",
      "Decoder Target Shape: (1000, 1768)\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1768) and (None, 1768, 24973) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16771/617928949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Modeli eğitin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_input_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_target_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/hduser/.local/lib/python3.10/site-packages/keras/backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1768) and (None, 1768, 24973) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Prepare decoder input data (remove the last token)\n",
    "decoder_input_data = train_highlights_seq[:, :-1]\n",
    "\n",
    "# Prepare decoder target data (remove the first token)\n",
    "decoder_target_data = train_highlights_seq[:, 1:]\n",
    "\n",
    "# Prepare training data\n",
    "encoder_input_data = train_articles_seq  # Articles as encoder inputs\n",
    "\n",
    "# Check the data dimensions\n",
    "print(\"Encoder Input Shape:\", encoder_input_data.shape)\n",
    "print(\"Decoder Input Shape:\", decoder_input_data.shape)\n",
    "print(\"Decoder Target Shape:\", decoder_target_data.shape)\n",
    "\n",
    "# Train the model\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=64, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
