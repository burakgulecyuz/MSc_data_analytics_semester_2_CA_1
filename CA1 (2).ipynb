{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b14d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb7c6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8df5e568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession oluşturma\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SummarizationApp\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# HDFS'teki CSV dosyalarını okuma\n",
    "train_data = spark.read.option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/train.csv\")\n",
    "test_data = spark.read.option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/test.csv\")\n",
    "validation_data = spark.read.option(\"header\", \"true\").csv(\"hdfs:///cnn_dailymail/validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9fc17ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             article|              tokens|     filtered_tokens|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|by  associated pr...|[by, , associated...|[, associated, pr...|\n",
      "| grand forks and ...|[, grand, forks, ...|[, grand, forks, ...|\n",
      "|cnn  ralph mata w...|[cnn, , ralph, ma...|[cnn, , ralph, ma...|\n",
      "| an internal affa...|[, an, internal, ...|[, internal, affa...|\n",
      "| a complaint alle...|[, a, complaint, ...|[, complaint, all...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# 1. 'id' ve 'highlights' sütunlarını kaldırma\n",
    "train_data = train_data.drop(\"id\", \"highlights\")\n",
    "test_data = test_data.drop(\"id\", \"highlights\")\n",
    "validation_data = validation_data.drop(\"id\", \"highlights\")\n",
    "\n",
    "# 2. Null verileri düşürme\n",
    "train_data = train_data.na.drop(subset=[\"article\"])\n",
    "test_data = test_data.na.drop(subset=[\"article\"])\n",
    "validation_data = validation_data.na.drop(subset=[\"article\"])\n",
    "\n",
    "# 3. Regex ile sadece alfabedik karakterleri ve boşlukları koruma\n",
    "train_data = train_data.withColumn(\"article\", regexp_replace(\"article\", \"[^a-zA-Z\\s]\", \"\"))\n",
    "test_data = test_data.withColumn(\"article\", regexp_replace(\"article\", \"[^a-zA-Z\\s]\", \"\"))\n",
    "validation_data = validation_data.withColumn(\"article\", regexp_replace(\"article\", \"[^a-zA-Z\\s]\", \"\"))\n",
    "\n",
    "# 4. Tüm harfleri küçük harfe çevirme\n",
    "train_data = train_data.withColumn(\"article\", lower(train_data[\"article\"]))\n",
    "test_data = test_data.withColumn(\"article\", lower(test_data[\"article\"]))\n",
    "validation_data = validation_data.withColumn(\"article\", lower(validation_data[\"article\"]))\n",
    "\n",
    "# 5. Tokenizasyon\n",
    "tokenizer = Tokenizer(inputCol=\"article\", outputCol=\"tokens\")\n",
    "train_data = tokenizer.transform(train_data)\n",
    "test_data = tokenizer.transform(test_data)\n",
    "validation_data = tokenizer.transform(validation_data)\n",
    "\n",
    "# 6. Stopwords kaldırma\n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
    "train_data = remover.transform(train_data)\n",
    "test_data = remover.transform(test_data)\n",
    "validation_data = remover.transform(validation_data)\n",
    "\n",
    "# Sonuçları kontrol etmek için\n",
    "train_data.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d94617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
